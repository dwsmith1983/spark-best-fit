{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark-dist-fit API Demo\n",
    "\n",
    "This notebook demonstrates the complete API for the `spark-dist-fit` library, including:\n",
    "\n",
    "1. **Configuration** - FitConfig, PlotConfig, SparkConfig, and AppConfig\n",
    "2. **Config Loading** - From code, strings, and files (HOCON/YAML/JSON)\n",
    "3. **Distribution Fitting** - Using DistributionFitter\n",
    "4. **Working with Results** - FitResults and FitResult objects\n",
    "5. **Plotting** - Visualization with PlotConfig\n",
    "6. **Convenience Methods** - One-line config loading with `from_config()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required modules and create a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/13 19:22:35 WARN Utils: Your hostname, MacBook-Pro-4.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.74 instead (on interface en0)\n",
      "25/12/13 19:22:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/13 19:22:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import all public API components\n",
    "from spark_dist_fit import (\n",
    "    DistributionFitter,\n",
    "    FitConfig,\n",
    "    PlotConfig,\n",
    "    SparkConfig,\n",
    "    AppConfig,\n",
    "    DEFAULT_EXCLUDED_DISTRIBUTIONS,\n",
    ")\n",
    "\n",
    "# Create Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"API-Demo\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create sample data from known distributions for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data: 50,000 rows, mean=50.00, std=10.00\n",
      "Exponential data: 50,000 rows, mean=4.97\n",
      "Gamma data: 50,000 rows, mean=4.01\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Normal distribution data\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=50_000)\n",
    "df_normal = spark.createDataFrame([(float(x),) for x in normal_data], [\"value\"])\n",
    "\n",
    "# Exponential distribution data (non-negative)\n",
    "exp_data = np.random.exponential(scale=5, size=50_000)\n",
    "df_exp = spark.createDataFrame([(float(x),) for x in exp_data], [\"value\"])\n",
    "\n",
    "# Gamma distribution data\n",
    "gamma_data = np.random.gamma(shape=2.0, scale=2.0, size=50_000)\n",
    "df_gamma = spark.createDataFrame([(float(x),) for x in gamma_data], [\"value\"])\n",
    "\n",
    "print(f\"Normal data: {df_normal.count():,} rows, mean={normal_data.mean():.2f}, std={normal_data.std():.2f}\")\n",
    "print(f\"Exponential data: {df_exp.count():,} rows, mean={exp_data.mean():.2f}\")\n",
    "print(f\"Gamma data: {df_gamma.count():,} rows, mean={gamma_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Configuration\n",
    "\n",
    "spark-dist-fit uses frozen dataclasses for type-safe, immutable configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 FitConfig - Distribution Fitting Configuration\n",
    "\n",
    "`FitConfig` controls histogram computation, sampling, and distribution selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default FitConfig:\n",
      "  bins: 50\n",
      "  use_rice_rule: True\n",
      "  support_at_zero: False\n",
      "  enable_sampling: True\n",
      "  sample_threshold: 10,000,000\n",
      "  max_sample_size: 1,000,000\n",
      "  max_sample_fraction: 0.35\n",
      "  random_seed: 42\n",
      "  excluded_distributions: 17 distributions\n"
     ]
    }
   ],
   "source": [
    "# Default configuration\n",
    "default_fit_config = FitConfig()\n",
    "print(\"Default FitConfig:\")\n",
    "print(f\"  bins: {default_fit_config.bins}\")\n",
    "print(f\"  use_rice_rule: {default_fit_config.use_rice_rule}\")\n",
    "print(f\"  support_at_zero: {default_fit_config.support_at_zero}\")\n",
    "print(f\"  enable_sampling: {default_fit_config.enable_sampling}\")\n",
    "print(f\"  sample_threshold: {default_fit_config.sample_threshold:,}\")\n",
    "print(f\"  max_sample_size: {default_fit_config.max_sample_size:,}\")\n",
    "print(f\"  max_sample_fraction: {default_fit_config.max_sample_fraction}\")\n",
    "print(f\"  random_seed: {default_fit_config.random_seed}\")\n",
    "print(f\"  excluded_distributions: {len(default_fit_config.excluded_distributions)} distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom FitConfig (for non-negative data):\n",
      "  bins: 100\n",
      "  support_at_zero: True\n",
      "  sample_fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Custom configuration\n",
    "custom_fit_config = FitConfig(\n",
    "    bins=100,                      # More bins for better resolution\n",
    "    use_rice_rule=False,           # Don't auto-calculate bins\n",
    "    support_at_zero=True,          # Only fit non-negative distributions\n",
    "    enable_sampling=True,          # Enable adaptive sampling\n",
    "    sample_fraction=0.3,           # Sample 30% of data\n",
    "    max_sample_size=500_000,       # Cap samples at 500K\n",
    "    random_seed=123,               # Custom seed for reproducibility\n",
    ")\n",
    "\n",
    "print(\"Custom FitConfig (for non-negative data):\")\n",
    "print(f\"  bins: {custom_fit_config.bins}\")\n",
    "print(f\"  support_at_zero: {custom_fit_config.support_at_zero}\")\n",
    "print(f\"  sample_fraction: {custom_fit_config.sample_fraction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default excluded distributions (17):\n",
      "  - exonpow\n",
      "  - gausshyper\n",
      "  - genhyperbolic\n",
      "  - geninvgauss\n",
      "  - kappa4\n",
      "  - ksone\n",
      "  - kstwo\n",
      "  - kstwobign\n",
      "  - levy_stable\n",
      "  - mielke\n",
      "  - ncf\n",
      "  - ncx2\n",
      "  - recipinvgauss\n",
      "  - studentized_range\n",
      "  - vonmises\n",
      "  - vonmises_line\n",
      "  - wald\n",
      "\n",
      "Now fitting 'wald' distribution (removed from exclusions)\n"
     ]
    }
   ],
   "source": [
    "# Customizing excluded distributions\n",
    "print(f\"\\nDefault excluded distributions ({len(DEFAULT_EXCLUDED_DISTRIBUTIONS)}):\")\n",
    "for dist in sorted(DEFAULT_EXCLUDED_DISTRIBUTIONS):\n",
    "    print(f\"  - {dist}\")\n",
    "\n",
    "# Include a specific distribution that's excluded by default\n",
    "custom_exclusions = tuple(d for d in DEFAULT_EXCLUDED_DISTRIBUTIONS if d != \"wald\")\n",
    "config_with_wald = FitConfig(excluded_distributions=custom_exclusions)\n",
    "print(f\"\\nNow fitting 'wald' distribution (removed from exclusions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PlotConfig - Visualization Configuration\n",
    "\n",
    "`PlotConfig` controls matplotlib figure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default PlotConfig:\n",
      "  figsize: (12, 8)\n",
      "  dpi: 600\n",
      "  show_histogram: True\n",
      "  histogram_alpha: 0.5\n",
      "  pdf_linewidth: 2\n",
      "  save_format: png\n"
     ]
    }
   ],
   "source": [
    "# Default PlotConfig\n",
    "default_plot_config = PlotConfig()\n",
    "print(\"Default PlotConfig:\")\n",
    "print(f\"  figsize: {default_plot_config.figsize}\")\n",
    "print(f\"  dpi: {default_plot_config.dpi}\")\n",
    "print(f\"  show_histogram: {default_plot_config.show_histogram}\")\n",
    "print(f\"  histogram_alpha: {default_plot_config.histogram_alpha}\")\n",
    "print(f\"  pdf_linewidth: {default_plot_config.pdf_linewidth}\")\n",
    "print(f\"  save_format: {default_plot_config.save_format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presentation PlotConfig:\n",
      "  figsize: (16, 10)\n",
      "  title_fontsize: 18\n"
     ]
    }
   ],
   "source": [
    "# Custom PlotConfig for presentations\n",
    "presentation_plot_config = PlotConfig(\n",
    "    figsize=(16, 10),\n",
    "    dpi=150,  # Lower DPI for notebook display\n",
    "    histogram_alpha=0.6,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    label_fontsize=14,\n",
    "    legend_fontsize=12,\n",
    "    grid_alpha=0.4,\n",
    ")\n",
    "\n",
    "print(\"Presentation PlotConfig:\")\n",
    "print(f\"  figsize: {presentation_plot_config.figsize}\")\n",
    "print(f\"  title_fontsize: {presentation_plot_config.title_fontsize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 SparkConfig - Spark Session Configuration\n",
    "\n",
    "`SparkConfig` manages Spark session settings for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default SparkConfig:\n",
      "  app_name: spark-dist-fit\n",
      "  arrow_enabled: True\n",
      "  adaptive_enabled: True\n",
      "  adaptive_coalesce_enabled: True\n",
      "\n",
      "Spark settings dict:\n",
      "  spark.sql.execution.arrow.pyspark.enabled: true\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n"
     ]
    }
   ],
   "source": [
    "# Default SparkConfig\n",
    "default_spark_config = SparkConfig()\n",
    "print(\"Default SparkConfig:\")\n",
    "print(f\"  app_name: {default_spark_config.app_name}\")\n",
    "print(f\"  arrow_enabled: {default_spark_config.arrow_enabled}\")\n",
    "print(f\"  adaptive_enabled: {default_spark_config.adaptive_enabled}\")\n",
    "print(f\"  adaptive_coalesce_enabled: {default_spark_config.adaptive_coalesce_enabled}\")\n",
    "\n",
    "# Convert to Spark config dict\n",
    "spark_settings = default_spark_config.to_spark_config()\n",
    "print(\"\\nSpark settings dict:\")\n",
    "for k, v in spark_settings.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Config Immutability\n",
    "\n",
    "All config classes are frozen dataclasses - they cannot be modified after creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot modify frozen config: cannot assign to field 'bins'\n",
      "\n",
      "Created new config with bins=200\n"
     ]
    }
   ],
   "source": [
    "# Attempt to modify config (will fail)\n",
    "try:\n",
    "    default_fit_config.bins = 200\n",
    "except AttributeError as e:\n",
    "    print(f\"Cannot modify frozen config: {e}\")\n",
    "\n",
    "# To change config values, create a new instance\n",
    "new_config = FitConfig(bins=200, support_at_zero=True)\n",
    "print(f\"\\nCreated new config with bins={new_config.bins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Config Loading\n",
    "\n",
    "Configs can be loaded from HOCON, YAML, or JSON files/strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Config from String (HOCON format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FitConfig from HOCON string:\n",
      "  bins: 75\n",
      "  sample_fraction: 0.25\n",
      "  random_seed: 99\n"
     ]
    }
   ],
   "source": [
    "# Load FitConfig from HOCON string (flat structure - no nesting)\n",
    "hocon_string = \"\"\"\n",
    "bins = 75\n",
    "use_rice_rule = false\n",
    "support_at_zero = false\n",
    "enable_sampling = true\n",
    "sample_fraction = 0.25\n",
    "random_seed = 99\n",
    "\"\"\"\n",
    "\n",
    "config_from_string = FitConfig.from_string(hocon_string)\n",
    "print(\"FitConfig from HOCON string:\")\n",
    "print(f\"  bins: {config_from_string.bins}\")\n",
    "print(f\"  sample_fraction: {config_from_string.sample_fraction}\")\n",
    "print(f\"  random_seed: {config_from_string.random_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlotConfig from HOCON string:\n",
      "  figsize: (14, 8)\n",
      "  dpi: 100\n"
     ]
    }
   ],
   "source": [
    "# Load PlotConfig from HOCON string\n",
    "plot_hocon = \"\"\"\n",
    "figsize = [14, 8]\n",
    "dpi = 100\n",
    "histogram_alpha = 0.7\n",
    "pdf_linewidth = 2\n",
    "title_fontsize = 16\n",
    "\"\"\"\n",
    "\n",
    "plot_config_from_string = PlotConfig.from_string(plot_hocon)\n",
    "print(\"PlotConfig from HOCON string:\")\n",
    "print(f\"  figsize: {plot_config_from_string.figsize}\")\n",
    "print(f\"  dpi: {plot_config_from_string.dpi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Nested Config with AppConfig\n",
    "\n",
    "For HOCON files with nested structure (`fit{}`, `plot{}`, `spark{}`), use `AppConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AppConfig from nested HOCON:\n",
      "  spark.app_name: demo-app\n",
      "  fit.bins: 80\n",
      "  fit.sample_fraction: 0.4\n",
      "  plot.figsize: (12, 8)\n"
     ]
    }
   ],
   "source": [
    "# Nested HOCON config (typical production format)\n",
    "nested_hocon = \"\"\"\n",
    "spark {\n",
    "    app_name = \"demo-app\"\n",
    "    arrow_enabled = true\n",
    "    adaptive_enabled = true\n",
    "}\n",
    "\n",
    "fit {\n",
    "    bins = 80\n",
    "    use_rice_rule = false\n",
    "    support_at_zero = false\n",
    "    enable_sampling = true\n",
    "    sample_fraction = 0.4\n",
    "    random_seed = 42\n",
    "}\n",
    "\n",
    "plot {\n",
    "    figsize = [12, 8]\n",
    "    dpi = 150\n",
    "    histogram_alpha = 0.5\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "app_config = AppConfig.from_string(nested_hocon)\n",
    "print(\"AppConfig from nested HOCON:\")\n",
    "print(f\"  spark.app_name: {app_config.spark.app_name}\")\n",
    "print(f\"  fit.bins: {app_config.fit.bins}\")\n",
    "print(f\"  fit.sample_fraction: {app_config.fit.sample_fraction}\")\n",
    "print(f\"  plot.figsize: {app_config.plot.figsize}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Config from File\n",
    "\n",
    "Load configuration from a file (HOCON, YAML, or JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AppConfig from file:\n",
      "  File: ../config/example.conf\n",
      "  spark.app_name: spark-dist-fit\n",
      "  fit.bins: 100\n",
      "  fit.excluded_distributions: 8 distributions\n",
      "  plot.dpi: 600\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load from repository example config\n",
    "config_path = Path(\"../config/example.conf\")\n",
    "\n",
    "if config_path.exists():\n",
    "    app_config_from_file = AppConfig.from_file(str(config_path))\n",
    "    print(\"AppConfig from file:\")\n",
    "    print(f\"  File: {config_path}\")\n",
    "    print(f\"  spark.app_name: {app_config_from_file.spark.app_name}\")\n",
    "    print(f\"  fit.bins: {app_config_from_file.fit.bins}\")\n",
    "    print(f\"  fit.excluded_distributions: {len(app_config_from_file.fit.excluded_distributions)} distributions\")\n",
    "    print(f\"  plot.dpi: {app_config_from_file.plot.dpi}\")\n",
    "else:\n",
    "    print(f\"Config file not found: {config_path}\")\n",
    "    print(\"Run this notebook from the examples/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Distribution Fitting\n",
    "\n",
    "The `DistributionFitter` class is the main entry point for fitting distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Basic Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting distributions to normal data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 19:22:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 26:====================================================>   (15 + 1) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitted 20 distributions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Create fitter with default config\n",
    "fitter = DistributionFitter()\n",
    "\n",
    "# Fit distributions to normal data (limit to 20 for demo speed)\n",
    "print(\"Fitting distributions to normal data...\")\n",
    "results_normal = fitter.fit(df_normal, column=\"value\", max_distributions=20)\n",
    "\n",
    "print(f\"\\nFitted {results_normal.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fitting with Custom Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting non-negative distributions to exponential data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:====================================================>   (14 + 1) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted 15 non-negative distributions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Config for non-negative distributions only\n",
    "nonneg_config = FitConfig(\n",
    "    bins=100,\n",
    "    support_at_zero=True,  # Only fit non-negative distributions\n",
    "    enable_sampling=True,\n",
    ")\n",
    "\n",
    "fitter_nonneg = DistributionFitter(config=nonneg_config)\n",
    "\n",
    "print(\"Fitting non-negative distributions to exponential data...\")\n",
    "results_exp = fitter_nonneg.fit(df_exp, column=\"value\", max_distributions=15)\n",
    "\n",
    "print(f\"Fitted {results_exp.count()} non-negative distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Using from_config() Convenience Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/13 19:23:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memory\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m config_path = Path(\u001b[33m\"\u001b[39m\u001b[33m../config/example.conf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_path.exists():\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Load fitter with all configs in one line\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     fitter_from_config = \u001b[43mDistributionFitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitter created from config file:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  fit.bins: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfitter_from_config.config.bins\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/spark_dist_fit/core.py:106\u001b[39m, in \u001b[36mDistributionFitter.from_config\u001b[39m\u001b[34m(cls, config_path, spark)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspark_dist_fit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AppConfig\n\u001b[32m    104\u001b[39m app_config = AppConfig.from_file(config_path)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m fitter = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapp_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspark_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapp_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Store plot config for convenient access\u001b[39;00m\n\u001b[32m    112\u001b[39m fitter.plot_config = app_config.plot\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/spark_dist_fit/core.py:69\u001b[39m, in \u001b[36mDistributionFitter.__init__\u001b[39m\u001b[34m(self, spark, config, spark_config, distribution_registry)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     56\u001b[39m     spark: Optional[SparkSession] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     distribution_registry: Optional[DistributionRegistry] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     60\u001b[39m ):\n\u001b[32m     61\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Initialize DistributionFitter.\u001b[39;00m\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[33;03m        distribution_registry: Custom distribution registry (uses default if None)\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m.config: FitConfig = config \u001b[38;5;129;01mor\u001b[39;00m FitConfig()\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m.registry: DistributionRegistry = distribution_registry \u001b[38;5;129;01mor\u001b[39;00m DistributionRegistry()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/spark_dist_fit/utils.py:52\u001b[39m, in \u001b[36mSparkSessionWrapper.__init__\u001b[39m\u001b[34m(self, spark, spark_config)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mself\u001b[39m._spark = builder.getOrCreate()\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Apply config to existing session (runtime settings)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_spark_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/spark_dist_fit/utils.py:57\u001b[39m, in \u001b[36mSparkSessionWrapper._apply_spark_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Apply Spark configuration to the session.\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark_config.to_spark_config().items():\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/pyspark/sql/conf.py:59\u001b[39m, in \u001b[36mRuntimeConfig.set\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mbool\u001b[39m]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     43\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m    Sets the given Spark runtime configuration property.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m \u001b[33;03m    >>> spark.conf.set(\"key1\", \"value1\")\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memory\".\nSee also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110"
     ]
    }
   ],
   "source": [
    "# One-liner to create fitter from config file\n",
    "config_path = Path(\"../config/example.conf\")\n",
    "\n",
    "if config_path.exists():\n",
    "    # Load fitter with all configs in one line\n",
    "    fitter_from_config = DistributionFitter.from_config(str(config_path))\n",
    "    \n",
    "    print(\"Fitter created from config file:\")\n",
    "    print(f\"  fit.bins: {fitter_from_config.config.bins}\")\n",
    "    print(f\"  plot_config available: {fitter_from_config.plot_config is not None}\")\n",
    "    \n",
    "    # The plot_config is automatically loaded\n",
    "    if fitter_from_config.plot_config:\n",
    "        print(f\"  plot.dpi: {fitter_from_config.plot_config.dpi}\")\n",
    "else:\n",
    "    print(\"Config file not found - run from examples/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Working with Results\n",
    "\n",
    "The `fit()` method returns a `FitResults` object for easy result manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Getting Best Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best distribution by SSE (default)\n",
    "best_sse = results_normal.best(n=1)[0]\n",
    "print(f\"Best by SSE: {best_sse.distribution}\")\n",
    "print(f\"  SSE: {best_sse.sse:.6f}\")\n",
    "print(f\"  AIC: {best_sse.aic:.2f}\")\n",
    "print(f\"  BIC: {best_sse.bic:.2f}\")\n",
    "print(f\"  Parameters: {[f'{p:.4f}' for p in best_sse.parameters]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 by different metrics\n",
    "print(\"\\nTop 5 by SSE:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"sse\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} SSE={r.sse:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 by AIC:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"aic\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} AIC={r.aic:.2f}\")\n",
    "\n",
    "print(\"\\nTop 5 by BIC:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"bic\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} BIC={r.bic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by SSE threshold\n",
    "good_fits = results_normal.filter(sse_threshold=0.01)\n",
    "print(f\"Distributions with SSE < 0.01: {good_fits.count()}\")\n",
    "\n",
    "for r in good_fits.best(n=10):\n",
    "    print(f\"  {r.distribution:20s} SSE={r.sse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Converting to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for further analysis\n",
    "df_results = results_normal.to_pandas()\n",
    "print(\"Results as pandas DataFrame:\")\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Using Fitted Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FitResult object wraps the scipy.stats distribution\n",
    "best = results_normal.best(n=1)[0]\n",
    "\n",
    "# Generate samples from the fitted distribution\n",
    "samples = best.sample(size=10000, random_state=42)\n",
    "print(f\"Generated {len(samples)} samples from fitted {best.distribution}\")\n",
    "print(f\"  Sample mean: {samples.mean():.2f} (original: {normal_data.mean():.2f})\")\n",
    "print(f\"  Sample std: {samples.std():.2f} (original: {normal_data.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PDF at specific points\n",
    "x = np.array([30, 40, 50, 60, 70])\n",
    "pdf_values = best.pdf(x)\n",
    "cdf_values = best.cdf(x)\n",
    "\n",
    "print(\"\\nPDF and CDF values:\")\n",
    "for xi, pdf, cdf in zip(x, pdf_values, cdf_values):\n",
    "    print(f\"  x={xi}: PDF={pdf:.6f}, CDF={cdf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Plotting\n",
    "\n",
    "Visualize the fitted distribution with the data histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot with default config\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    title=\"Best Fit Distribution (Normal Data)\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Plot with Custom PlotConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom plot configuration\n",
    "custom_plot_config = PlotConfig(\n",
    "    figsize=(14, 8),\n",
    "    dpi=100,\n",
    "    histogram_alpha=0.7,\n",
    "    pdf_linewidth=3,\n",
    "    title_fontsize=18,\n",
    "    label_fontsize=14,\n",
    "    legend_fontsize=12,\n",
    "    grid_alpha=0.4,\n",
    ")\n",
    "\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    config=custom_plot_config,\n",
    "    title=\"Distribution Fit with Custom Styling\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Plot Non-Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best fit for exponential data\n",
    "best_exp = results_exp.best(n=1)[0]\n",
    "print(f\"Best fit for exponential data: {best_exp.distribution}\")\n",
    "\n",
    "fig, ax = fitter_nonneg.plot(\n",
    "    best_exp,\n",
    "    df_exp,\n",
    "    \"value\",\n",
    "    config=custom_plot_config,\n",
    "    title=f\"Best Fit: {best_exp.distribution.capitalize()}\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Complete Workflow Example\n",
    "\n",
    "Putting it all together - a complete production-style workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config as HOCON string (could also be loaded from file)\n",
    "production_config = \"\"\"\n",
    "spark {\n",
    "    app_name = \"production-fitting\"\n",
    "    arrow_enabled = true\n",
    "    adaptive_enabled = true\n",
    "}\n",
    "\n",
    "fit {\n",
    "    bins = 100\n",
    "    use_rice_rule = false\n",
    "    support_at_zero = false\n",
    "    enable_sampling = true\n",
    "    max_sample_size = 1000000\n",
    "    random_seed = 42\n",
    "}\n",
    "\n",
    "plot {\n",
    "    figsize = [14, 9]\n",
    "    dpi = 150\n",
    "    histogram_alpha = 0.6\n",
    "    pdf_linewidth = 3\n",
    "    title_fontsize = 16\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Load config\n",
    "config = AppConfig.from_string(production_config)\n",
    "\n",
    "# Create fitter with config\n",
    "fitter = DistributionFitter(config=config.fit, spark_config=config.spark)\n",
    "\n",
    "# Fit distributions\n",
    "print(\"Fitting gamma distribution data...\")\n",
    "results = fitter.fit(df_gamma, column=\"value\", max_distributions=25)\n",
    "\n",
    "# Get best result\n",
    "best = results.best(n=1)[0]\n",
    "print(f\"\\nBest distribution: {best.distribution}\")\n",
    "print(f\"SSE: {best.sse:.6f}\")\n",
    "print(f\"Parameters: {[f'{p:.4f}' for p in best.parameters]}\")\n",
    "\n",
    "# Plot with config\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_gamma,\n",
    "    \"value\",\n",
    "    config=config.plot,\n",
    "    title=f\"Gamma Data - Best Fit: {best.distribution.capitalize()}\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Show top 5 results\n",
    "print(\"\\nTop 5 distributions:\")\n",
    "df_top5 = results.to_pandas().head(5)\n",
    "df_top5[[\"distribution\", \"sse\", \"aic\", \"bic\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Configuration Classes**:\n",
    "   - `FitConfig` - Distribution fitting parameters\n",
    "   - `PlotConfig` - Visualization settings\n",
    "   - `SparkConfig` - Spark session configuration\n",
    "   - `AppConfig` - Container for nested configs\n",
    "\n",
    "2. **Config Loading**:\n",
    "   - `FitConfig()` - Direct instantiation\n",
    "   - `FitConfig.from_string()` - From HOCON/YAML/JSON string\n",
    "   - `AppConfig.from_file()` - From nested config file\n",
    "   - `DistributionFitter.from_config()` - One-liner convenience method\n",
    "\n",
    "3. **Fitting**:\n",
    "   - `DistributionFitter.fit()` - Fit distributions to data\n",
    "   - `max_distributions` parameter to limit fitting scope\n",
    "   - `support_at_zero` for non-negative data\n",
    "\n",
    "4. **Results**:\n",
    "   - `results.best(n, metric)` - Get top N by SSE/AIC/BIC\n",
    "   - `results.filter()` - Filter by threshold\n",
    "   - `results.to_pandas()` - Convert to pandas DataFrame\n",
    "   - `FitResult.sample()`, `.pdf()`, `.cdf()` - Use fitted distribution\n",
    "\n",
    "5. **Plotting**:\n",
    "   - `fitter.plot()` - Visualize fitted distribution with data histogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

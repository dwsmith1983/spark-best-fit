# Example HOCON configuration for spark-dist-fit
# This format also supports YAML and JSON

# Spark session configuration
spark {
    app_name = "spark-dist-fit"
    arrow_enabled = true       # Enable Arrow for Pandas UDFs (recommended)
    adaptive_enabled = true    # Enable Adaptive Query Execution
    adaptive_coalesce_enabled = true  # Enable adaptive partition coalescing

    # Additional Spark configs can be added here
     extra_config {
         "spark.executor.memory" = "4g"
         "spark.driver.memory" = "2g"
     }
}

# Distribution fitting configuration
fit {
    # Histogram configuration
    bins = 100
    use_rice_rule = false  # Set to true to auto-calculate bins using Rice rule

    # Distribution filtering
    support_at_zero = false  # Set to true to only fit non-negative distributions
    excluded_distributions = [
        "levy_stable",  # Very slow
        "kappa4",       # Very slow
        "ncx2",         # Slow
        "ksone",        # Slow
        "ncf",          # Slow
        "wald",         # Sometimes unstable
        "mielke",       # Slow
        "exonpow"       # Slow
    ]

    # Sampling configuration (for large datasets)
    enable_sampling = true
    sample_fraction = null  # null = auto-determine based on data size
    max_sample_size = 1000000
    sample_threshold = 10000000  # Sample when row count exceeds this

    # Spark configuration
    num_partitions = null  # null = auto-determine based on cluster size

    # Reproducibility
    random_seed = 42
}

# Plotting configuration
plot {
    figsize = [12, 8]
    dpi = 600
    show_histogram = true
    histogram_alpha = 0.5
    pdf_linewidth = 2
    save_format = "png"  # png, pdf, or svg
    title_fontsize = 14
    label_fontsize = 12
    legend_fontsize = 10
    grid_alpha = 0.3
}
